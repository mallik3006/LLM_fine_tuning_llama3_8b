{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T02:21:34.817039Z","iopub.status.busy":"2024-05-27T02:21:34.816543Z","iopub.status.idle":"2024-05-27T02:22:27.699622Z","shell.execute_reply":"2024-05-27T02:22:27.698566Z","shell.execute_reply.started":"2024-05-27T02:21:34.817003Z"},"trusted":true},"outputs":[],"source":["!pip install -qqq -U transformers datasets accelerate peft trl bitsandbytes deepspeed --progress-bar off"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T02:22:27.702026Z","iopub.status.busy":"2024-05-27T02:22:27.701707Z","iopub.status.idle":"2024-05-27T02:22:27.715118Z","shell.execute_reply":"2024-05-27T02:22:27.714353Z","shell.execute_reply.started":"2024-05-27T02:22:27.701999Z"},"trusted":true},"outputs":[],"source":["import gc\n","import os\n","import json\n","from kaggle_secrets import UserSecretsClient"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T02:22:31.597430Z","iopub.status.busy":"2024-05-27T02:22:31.597045Z","iopub.status.idle":"2024-05-27T02:22:31.843932Z","shell.execute_reply":"2024-05-27T02:22:31.842983Z","shell.execute_reply.started":"2024-05-27T02:22:31.597404Z"},"trusted":true},"outputs":[],"source":["# Get keys from Secrets\n","user_secrets = UserSecretsClient()\n","HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","from accelerate.utils import write_basic_config\n","\n","write_basic_config()  # Write a config file\n","os._exit(00)  # Restart the notebook"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T04:45:38.391151Z","iopub.status.busy":"2024-05-27T04:45:38.390446Z","iopub.status.idle":"2024-05-27T04:45:38.398619Z","shell.execute_reply":"2024-05-27T04:45:38.397524Z","shell.execute_reply.started":"2024-05-27T04:45:38.391113Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Device: cuda\n","CUDA Version: 12.1\n","Pytorch 2.1.2\n","Num CPUs: 4\n","Num GPUs: 2\n","GPU Type: Tesla T4\n"]}],"source":["DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","print(f\"Device: {DEVICE}\")\n","print(f\"CUDA Version: {torch.version.cuda}\")\n","print(f\"Pytorch {torch.__version__}\")\n","\n","# Check the type and quantity of GPUs\n","if torch.cuda.is_available():\n","    print('Num CPUs:', os.cpu_count())\n","    print('Num GPUs:', torch.cuda.device_count())\n","    print('GPU Type:', torch.cuda.get_device_name(0))\n"]},{"cell_type":"markdown","metadata":{},"source":["### Llama_3_8b"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T02:22:31.592059Z","iopub.status.busy":"2024-05-27T02:22:31.591614Z","iopub.status.idle":"2024-05-27T02:22:31.595941Z","shell.execute_reply":"2024-05-27T02:22:31.595120Z","shell.execute_reply.started":"2024-05-27T02:22:31.592034Z"},"trusted":true},"outputs":[],"source":["# Model\n","base_model = \"meta-llama/Meta-Llama-3-8B\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dataset_name = \"mlabonne/orpo-dpo-mix-40k\"\n","dataset = load_dataset(dataset_name, split=\"all\")\n","dataset = dataset.shuffle(seed=42).select(range(100)) # Only use 1000 samples for quick demo\n","\n","def format_chat_template(row):\n","    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n","    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n","    return row\n","\n","dataset = dataset.map(\n","    format_chat_template,\n","    num_proc= os.cpu_count(),\n",")\n","dataset = dataset.train_test_split(test_size=0.01)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T02:22:27.720338Z","iopub.status.busy":"2024-05-27T02:22:27.720052Z","iopub.status.idle":"2024-05-27T02:22:31.590566Z","shell.execute_reply":"2024-05-27T02:22:31.589699Z","shell.execute_reply.started":"2024-05-27T02:22:27.720305Z"},"trusted":true},"outputs":[],"source":["from accelerate import notebook_launcher\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    TrainingArguments,\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T02:28:21.292606Z","iopub.status.busy":"2024-05-27T02:28:21.292259Z","iopub.status.idle":"2024-05-27T02:28:21.309978Z","shell.execute_reply":"2024-05-27T02:28:21.309010Z","shell.execute_reply.started":"2024-05-27T02:28:21.292580Z"},"trusted":true},"outputs":[],"source":["\n","def main():\n","    \n","    from transformers import BitsAndBytesConfig\n","    from trl import ORPOConfig, ORPOTrainer, setup_chat_format\n","    from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n","    from accelerate import Accelerator\n","\n","    accelerator = Accelerator(mixed_precision='fp16')\n","#     accelerator = Accelerator()\n","    \n","    device_map = {\"\": accelerator.process_index}\n","#     device_map = {\"\": \"cuda:\" + str(int(os.environ.get(\"LOCAL_RANK\") or 0))}\n","#     device_map={'':torch.cuda.current_device()}\n","\n","    \n","    # QLoRA config\n","    bnb_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_compute_dtype=torch.float16,\n","        bnb_4bit_use_double_quant=True,\n","        bnb_4bit_quant_storage=torch.float16,\n","    )\n","\n","    # LoRA config\n","    peft_config = LoraConfig(\n","        r=8,\n","        lora_alpha=16,\n","        lora_dropout=0.1,\n","        bias=\"none\",\n","        task_type=\"CAUSAL_LM\",\n","#         target_modules=[\"all_linear\"],\n","        target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n","    )\n","    \n","    base_model = \"meta-llama/Meta-Llama-3-8B\"\n","    new_model = \"Llama-3-8B_FT_ORPO_DDP\"\n","    \n","    tokenizer = AutoTokenizer.from_pretrained(base_model, token=HF_TOKEN)\n","\n","    # Load model\n","    model = AutoModelForCausalLM.from_pretrained(\n","        base_model,\n","        quantization_config=bnb_config,\n","#         device_map=\"auto\",\n","        device_map=device_map,\n","        token=HF_TOKEN,\n","        attn_implementation=\"eager\",\n","        torch_dtype=torch.float16,\n","    )\n","    \n","    model, tokenizer = setup_chat_format(model, tokenizer)\n","    model = prepare_model_for_kbit_training(model)\n","    \n","    dataset_name = \"mlabonne/orpo-dpo-mix-40k\"\n","    dataset = load_dataset(dataset_name, split=\"all\")\n","    dataset = dataset.shuffle(seed=42).select(range(900)) # Only use 30 samples for test\n","\n","    def format_chat_template(row):\n","        row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n","        row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n","        return row\n","\n","    dataset = dataset.map(\n","        format_chat_template,\n","        num_proc= os.cpu_count(),\n","    )\n","    dataset = dataset.train_test_split(test_size=0.01)\n","    \n","    orpo_args = ORPOConfig(\n","        learning_rate=8e-6,\n","        lr_scheduler_type=\"linear\",\n","        max_length=1024,\n","        max_prompt_length=512,\n","        beta=0.1,\n","        per_device_train_batch_size=1,\n","        per_device_eval_batch_size=1,\n","        gradient_accumulation_steps=4,\n","        optim=\"paged_adamw_8bit\",\n","        num_train_epochs=1,\n","        evaluation_strategy=\"steps\",\n","        eval_strategy=\"steps\",\n","        eval_steps=0.2,\n","        logging_steps=1,\n","        warmup_steps=10,\n","        report_to=\"none\",\n","        output_dir=\"./results/\",\n","        remove_unused_columns=False,\n","        ddp_find_unused_parameters=False,\n","        gradient_checkpointing=True,\n","        gradient_checkpointing_kwargs = {\"use_reentrant\": True}, #must be false for DDP\n","    )\n","\n","    trainer = ORPOTrainer(\n","        model=model,\n","        args=orpo_args,\n","        train_dataset=dataset[\"train\"],\n","        eval_dataset=dataset[\"test\"],\n","        peft_config=peft_config,\n","        tokenizer=tokenizer,\n","    )\n","\n","    print(device_map)\n","    print(f'n_gpu: {orpo_args.n_gpu}; Mode: {orpo_args.parallel_mode}')\n","    print(f'Num Processes: {accelerator.num_processes}; Device: {accelerator.device}; Process Index: {accelerator.process_index}')\n","    print(f'Accel Type: {accelerator.distributed_type}')\n","\n","    \n","    trainer.train()\n","    trainer.save_model(new_model)\n","    "]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T02:28:28.032065Z","iopub.status.busy":"2024-05-27T02:28:28.031412Z","iopub.status.idle":"2024-05-27T04:40:13.979237Z","shell.execute_reply":"2024-05-27T04:40:13.977819Z","shell.execute_reply.started":"2024-05-27T02:28:28.032031Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Launching training on 2 GPUs.\n"]},{"name":"stderr","output_type":"stream","text":["2024-05-27 02:28:31.465156: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-27 02:28:31.465160: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-27 02:28:31.465225: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-27 02:28:31.465296: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-27 02:28:31.616930: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-05-27 02:28:31.616938: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["[2024-05-27 02:28:39,219] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","[2024-05-27 02:28:39,219] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n","\n","\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n","\n","\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n","\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n","\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n","\n","\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n","\n","\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n","\u001b[93m [WARNING] \u001b[0m please install triton==1.0.0 if you want to use sparse attention\n","\n","\u001b[93m [WARNING] \u001b[0m please install triton==1.0.0 if you want to use sparse attention\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/compiler_compat/ld: cannot find -laio: No such file or directory\n","/opt/conda/compiler_compat/ld: cannot find -laio: No such file or directory\n","collect2: error: ld returned 1 exit status\n","collect2: error: ld returned 1 exit status\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e52f5b1556834ff0af2cf537a8f67b42","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a2126fe8ec4045f08cf65e06125b0c70","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["{'': 0}\n","n_gpu: 1; Mode: ParallelMode.DISTRIBUTED\n","Num Processes: 2; Device: cuda:0; Process Index: 0\n","Accel Type: MULTI_GPU\n","{'': 1}\n","n_gpu: 1; Mode: ParallelMode.DISTRIBUTED\n","Num Processes: 2; Device: cuda:1; Process Index: 1\n","Accel Type: MULTI_GPU\n"]},{"name":"stderr","output_type":"stream","text":["Could not estimate the number of tokens of the input, floating-point operations will not be computed\n","Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='111' max='111' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [111/111 2:09:44, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Runtime</th>\n","      <th>Samples Per Second</th>\n","      <th>Steps Per Second</th>\n","      <th>Rewards/chosen</th>\n","      <th>Rewards/rejected</th>\n","      <th>Rewards/accuracies</th>\n","      <th>Rewards/margins</th>\n","      <th>Logps/rejected</th>\n","      <th>Logps/chosen</th>\n","      <th>Logits/rejected</th>\n","      <th>Logits/chosen</th>\n","      <th>Nll Loss</th>\n","      <th>Log Odds Ratio</th>\n","      <th>Log Odds Chosen</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>23</td>\n","      <td>1.723900</td>\n","      <td>1.881545</td>\n","      <td>27.230300</td>\n","      <td>0.331000</td>\n","      <td>0.184000</td>\n","      <td>-0.106914</td>\n","      <td>-0.134332</td>\n","      <td>0.800000</td>\n","      <td>0.027418</td>\n","      <td>-1.343322</td>\n","      <td>-1.069142</td>\n","      <td>-1.505002</td>\n","      <td>-1.159490</td>\n","      <td>1.610141</td>\n","      <td>-0.541349</td>\n","      <td>0.384480</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>1.345600</td>\n","      <td>1.431592</td>\n","      <td>27.164800</td>\n","      <td>0.331000</td>\n","      <td>0.184000</td>\n","      <td>-0.099002</td>\n","      <td>-0.123506</td>\n","      <td>0.800000</td>\n","      <td>0.024504</td>\n","      <td>-1.235061</td>\n","      <td>-0.990021</td>\n","      <td>-1.605106</td>\n","      <td>-1.223708</td>\n","      <td>1.175749</td>\n","      <td>-0.552438</td>\n","      <td>0.356117</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>1.240000</td>\n","      <td>1.378136</td>\n","      <td>26.974800</td>\n","      <td>0.334000</td>\n","      <td>0.185000</td>\n","      <td>-0.094669</td>\n","      <td>-0.117788</td>\n","      <td>0.800000</td>\n","      <td>0.023119</td>\n","      <td>-1.177883</td>\n","      <td>-0.946691</td>\n","      <td>-1.582639</td>\n","      <td>-1.224063</td>\n","      <td>1.133539</td>\n","      <td>-0.557794</td>\n","      <td>0.341140</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>1.312100</td>\n","      <td>1.350144</td>\n","      <td>26.862300</td>\n","      <td>0.335000</td>\n","      <td>0.186000</td>\n","      <td>-0.091728</td>\n","      <td>-0.114803</td>\n","      <td>0.800000</td>\n","      <td>0.023074</td>\n","      <td>-1.148027</td>\n","      <td>-0.917283</td>\n","      <td>-1.559889</td>\n","      <td>-1.197512</td>\n","      <td>1.107633</td>\n","      <td>-0.555265</td>\n","      <td>0.347198</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='111' max='111' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [111/111 2:09:44, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Runtime</th>\n","      <th>Samples Per Second</th>\n","      <th>Steps Per Second</th>\n","      <th>Rewards/chosen</th>\n","      <th>Rewards/rejected</th>\n","      <th>Rewards/accuracies</th>\n","      <th>Rewards/margins</th>\n","      <th>Logps/rejected</th>\n","      <th>Logps/chosen</th>\n","      <th>Logits/rejected</th>\n","      <th>Logits/chosen</th>\n","      <th>Nll Loss</th>\n","      <th>Log Odds Ratio</th>\n","      <th>Log Odds Chosen</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>23</td>\n","      <td>1.723900</td>\n","      <td>1.881545</td>\n","      <td>27.232400</td>\n","      <td>0.330000</td>\n","      <td>0.184000</td>\n","      <td>-0.129463</td>\n","      <td>-0.134168</td>\n","      <td>0.600000</td>\n","      <td>0.004705</td>\n","      <td>-1.341682</td>\n","      <td>-1.294635</td>\n","      <td>-1.724199</td>\n","      <td>-1.461578</td>\n","      <td>1.903299</td>\n","      <td>-0.662111</td>\n","      <td>0.070167</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>1.345600</td>\n","      <td>1.431592</td>\n","      <td>27.167000</td>\n","      <td>0.331000</td>\n","      <td>0.184000</td>\n","      <td>-0.120951</td>\n","      <td>-0.124823</td>\n","      <td>0.600000</td>\n","      <td>0.003872</td>\n","      <td>-1.248229</td>\n","      <td>-1.209508</td>\n","      <td>-1.747398</td>\n","      <td>-1.485025</td>\n","      <td>1.514651</td>\n","      <td>-0.667016</td>\n","      <td>0.061171</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>1.240000</td>\n","      <td>1.378136</td>\n","      <td>26.977800</td>\n","      <td>0.334000</td>\n","      <td>0.185000</td>\n","      <td>-0.115477</td>\n","      <td>-0.120255</td>\n","      <td>0.600000</td>\n","      <td>0.004778</td>\n","      <td>-1.202549</td>\n","      <td>-1.154765</td>\n","      <td>-1.724360</td>\n","      <td>-1.488208</td>\n","      <td>1.450350</td>\n","      <td>-0.659136</td>\n","      <td>0.079157</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>1.312100</td>\n","      <td>1.350144</td>\n","      <td>26.865200</td>\n","      <td>0.335000</td>\n","      <td>0.186000</td>\n","      <td>-0.112037</td>\n","      <td>-0.117149</td>\n","      <td>0.600000</td>\n","      <td>0.005112</td>\n","      <td>-1.171488</td>\n","      <td>-1.120369</td>\n","      <td>-1.706816</td>\n","      <td>-1.472296</td>\n","      <td>1.419418</td>\n","      <td>-0.655846</td>\n","      <td>0.087071</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66540ead-045fd4063cda7600755be5c1;1c8a9163-6b6a-4af8-a9fa-0015fafb7728)\n","\n","Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n","Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 692 ms, sys: 258 ms, total: 950 ms\n","Wall time: 2h 11min 45s\n"]}],"source":["%%time\n","\n","notebook_launcher(main, num_processes=2)"]},{"cell_type":"markdown","metadata":{},"source":["### Merge Adapter with Base model"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T04:42:48.804060Z","iopub.status.busy":"2024-05-27T04:42:48.803155Z","iopub.status.idle":"2024-05-27T04:42:48.953600Z","shell.execute_reply":"2024-05-27T04:42:48.952255Z","shell.execute_reply.started":"2024-05-27T04:42:48.804027Z"},"trusted":true},"outputs":[],"source":["# Flush memory\n","# del trainer, model\n","gc.collect()\n","torch.cuda.empty_cache()\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T04:44:03.284947Z","iopub.status.busy":"2024-05-27T04:44:03.284032Z","iopub.status.idle":"2024-05-27T04:44:20.647385Z","shell.execute_reply":"2024-05-27T04:44:20.646376Z","shell.execute_reply.started":"2024-05-27T04:44:03.284913Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[2024-05-27 04:44:04,625] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n","\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n","\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n","\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n","\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/compiler_compat/ld: cannot find -laio: No such file or directory\n","collect2: error: ld returned 1 exit status\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n","\u001b[93m [WARNING] \u001b[0m please install triton==1.0.0 if you want to use sparse attention\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ba3419c1ce2f43dd87d4d2131210c22f","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Reload tokenizer and model\n","\n","from trl import setup_chat_format\n","\n","tokenizer = AutoTokenizer.from_pretrained(base_model, token=HF_TOKEN)\n","fp16_model = AutoModelForCausalLM.from_pretrained(\n","    base_model,\n","    low_cpu_mem_usage=True,\n","    return_dict=True,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n","    token=HF_TOKEN,\n",")\n","fp16_model, tokenizer = setup_chat_format(fp16_model, tokenizer)\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T04:44:47.636913Z","iopub.status.busy":"2024-05-27T04:44:47.636240Z","iopub.status.idle":"2024-05-27T04:44:48.617194Z","shell.execute_reply":"2024-05-27T04:44:48.616159Z","shell.execute_reply.started":"2024-05-27T04:44:47.636879Z"},"trusted":true},"outputs":[],"source":["# merge fine tuned adapter\n","from peft import PeftModel\n","\n","new_model = '/kaggle/working/Llama-3-8B_FT_ORPO_DDP'\n","\n","# Merge adapter with base model\n","model = PeftModel.from_pretrained(fp16_model, new_model)\n","model = model.merge_and_unload()"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T04:44:52.576355Z","iopub.status.busy":"2024-05-27T04:44:52.575343Z","iopub.status.idle":"2024-05-27T04:44:52.586057Z","shell.execute_reply":"2024-05-27T04:44:52.585056Z","shell.execute_reply.started":"2024-05-27T04:44:52.576316Z"},"trusted":true},"outputs":[{"data":{"text/plain":["LlamaForCausalLM(\n","  (model): LlamaModel(\n","    (embed_tokens): Embedding(128258, 4096)\n","    (layers): ModuleList(\n","      (0-31): 32 x LlamaDecoderLayer(\n","        (self_attn): LlamaSdpaAttention(\n","          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n","          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n","          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n","          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n","          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n","          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): LlamaRMSNorm()\n","        (post_attention_layernorm): LlamaRMSNorm()\n","      )\n","    )\n","    (norm): LlamaRMSNorm()\n","  )\n","  (lm_head): Linear(in_features=4096, out_features=128258, bias=False)\n",")"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T05:06:06.882771Z","iopub.status.busy":"2024-05-27T05:06:06.882307Z","iopub.status.idle":"2024-05-27T05:07:02.171261Z","shell.execute_reply":"2024-05-27T05:07:02.170371Z","shell.execute_reply.started":"2024-05-27T05:06:06.882737Z"},"trusted":true},"outputs":[],"source":["model.save_pretrained('/kaggle/working/model')"]},{"cell_type":"markdown","metadata":{},"source":["### Inference with Fine-tuned model"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T04:47:33.265667Z","iopub.status.busy":"2024-05-27T04:47:33.264720Z","iopub.status.idle":"2024-05-27T04:47:50.883892Z","shell.execute_reply":"2024-05-27T04:47:50.882840Z","shell.execute_reply.started":"2024-05-27T04:47:33.265630Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["generated_text:  user\n","What is the basic structure of a SQL query to join to tables on a field like ID\n","assistant\n","What is the basic structure of a SQL query to join to tables on a field like ID?\n","I have a table called `users` and a table called `user_achievements`. The `user_achievements` table has an `achievement_id` field and a `user_id` field. The `users` table has an `id` field. I want to select the `achievement_id` and `achievement_name` from the `user_achievements` table and the `name` and `id` fields from the `users` table. The `user_id` field in the `user_achievements` table is a foreign key to the `id` field in the `users` table. How do I write the query to join the two tables? I think I need a join, but I'm not sure how to write it. Can someone help me out?\n","I think you're looking for something like this:\n","    users.name,\n","    users.id,\n","    user_achievements.achievement_id,\n","    user_achievements.achievement_name\n","FROM users\n","INNER JOIN user_achievements ON users.id = user_achievements.user_id\n","This will give you the name and id of the user, as well as the achievement_id and achievement_name\n","CPU times: user 17.6 s, sys: 0 ns, total: 17.6 s\n","Wall time: 17.6 s\n"]}],"source":["%%time\n","question = 'What is the basic structure of a SQL query to join to tables on a field like ID'\n","# question = 'When is labor day celebrated in USA'\n","# question = 'When is the American Independence day'\n","\n","# Create the prompt\n","prompt = f\"\"\"<|im_start|>user\n","{question}<|im_end|>\n","<|im_start|>assistant\n","\"\"\"\n","\n","# Tokenize the prompt\n","inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n","# Generate the outputs from prompt\n","generate_ids = model.generate(**inputs, max_new_tokens=256)\n","# Decode the generated output\n","generated_text = tokenizer.batch_decode(generate_ids,\n","                                    skip_special_tokens=True,\n","                                    clean_up_tokenization_spaces=False\n","                                       )[0]\n","\n","print('generated_text: ', generated_text)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T04:50:15.461303Z","iopub.status.busy":"2024-05-27T04:50:15.460919Z","iopub.status.idle":"2024-05-27T04:50:24.231378Z","shell.execute_reply":"2024-05-27T04:50:24.230423Z","shell.execute_reply.started":"2024-05-27T04:50:15.461272Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["generated_text:  user\n","When is the American Independence day\n","assistant\n","When is the American Independence day\n","The American Independence Day is a holiday that is celebrated in the United States on July 4th every year. It commemorates the adoption of the Declaration of Independence in 1776. The Declaration of Independence was a document that declared the thirteen colonies of the United States to be independent from the British Empire. It was signed by the Continental Congress, a group of representatives from the colonies, on July 4, 1776.\n","The American Independence Day is a day of celebration and remembrance. It is a time to reflect on the history and values of the United States, and to celebrate the freedoms and\n","CPU times: user 8.77 s, sys: 2.06 ms, total: 8.77 s\n","Wall time: 8.76 s\n"]}],"source":["%%time\n","system_message = 'You are a smart assistant, answer the following question'\n","question = 'When is the American Independence day'\n","# question = 'When is labor day celebrated in USA'\n","\n","# Create the prompt\n","prompt = f\"\"\"<|im_start|>user\n","{question}<|im_end|>\n","<|im_start|>assistant\n","\"\"\"\n","\n","# Tokenize the prompt\n","inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n","# Generate the outputs from prompt\n","generate_ids = model.generate(**inputs, max_new_tokens=128)\n","# Decode the generated output\n","generated_text = tokenizer.batch_decode(generate_ids,\n","                                    skip_special_tokens=True,\n","                                    clean_up_tokenization_spaces=False)[0]\n","\n","print('generated_text: ', generated_text)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5023400,"sourceId":8434268,"sourceType":"datasetVersion"},{"datasetId":5024842,"sourceId":8436265,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
